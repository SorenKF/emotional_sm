{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('emsub': conda)",
   "display_name": "Python 3.8.5 64-bit ('emsub': conda)",
   "metadata": {
    "interpreter": {
     "hash": "c8a63e31ffb1a3adb15a5de85a1df3b507cd69b37963f70a13abe50a567902a6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Concate train and test files."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all TRAC test and gold data. Whoops...\n",
    "\n",
    "\n",
    "trac_fb = pd.read_csv('../Data/TRAC2018_full/converted/trac-gold-set/agr_en_fb_gold.csv')\n",
    "trac_sm = pd.read_csv('../Data/TRAC2018_full/converted/trac-gold-set/agr_en_tw_gold.csv')\n",
    "\n",
    "frames = [trac_fb, trac_sm]\n",
    "df_trac_test_combined = pd.concat(frames)\n",
    "\n",
    "\n",
    "df_trac_test_combined.to_csv('../Data/TRAC2018_full/converted/trac-gold-set/agr_en_combined_gold.csv', index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['text', 'label']",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1aa86929d324>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#pd.read_csv('../Data/gibert_vua_format/trainData.csv', sep='\\t')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../Data/hate_speech_mlma/MLMA_convert/level1/test.tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\emsub\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\emsub\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\emsub\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\emsub\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\emsub\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2052\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morig_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2053\u001b[0m             ):\n\u001b[1;32m-> 2054\u001b[1;33m                 \u001b[0m_validate_usecols_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morig_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2055\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2056\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\emsub\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_validate_usecols_names\u001b[1;34m(usecols, names)\u001b[0m\n\u001b[0;32m   1300\u001b[0m     \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0musecols\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1303\u001b[0m             \u001b[1;34mf\"Usecols do not match columns, columns expected but not found: {missing}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['text', 'label']"
     ]
    }
   ],
   "source": [
    "#pd.read_csv('../Data/gibert_vua_format/trainData.csv', sep='\\t')\n",
    "pd.read_csv('../Data/hate_speech_mlma/MLMA_convert/level1/test.tsv', sep='\\t', usecols=['text','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nFalse\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def has_duplicated(l):\n",
    "    return pd.Series(l).duplicated().sum() > 0\n",
    "\n",
    "print(has_duplicated(['one', 'two', 'one']))\n",
    "# True\n",
    "print(has_duplicated(['one', 'two', 'three']))\n",
    "# False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_data(train_file, test_file, type='csv'):\n",
    "    '''Concat and convert VUA data to pickle files for Deepmoji.\n",
    "\n",
    "    Args: \n",
    "        train_file (str): filepath to a training datafile.\n",
    "        test_file (str): filepath to a test_file.\n",
    "    \n",
    "    Optional:\n",
    "        type (str): specify data file format (default is .csv)\n",
    "\n",
    "    '''\n",
    "\n",
    "    # 1. Load the data\n",
    "\n",
    "    cols_to_use = ['text', 'label']\n",
    "    \n",
    "    # df_train = pd.read_csv(train_file, usecols=cols_to_use) \n",
    "    # df_test = pd.read_csv(test_file, usecols=cols_to_use)\n",
    "\n",
    "    df_train = pd.read_csv(train_file, sep='\\t', usecols=cols_to_use)\n",
    "    df_test = pd.read_csv(test_file, sep='\\t', usecols=cols_to_use)\n",
    "\n",
    "    #print(len(df_train), len(df_test))\n",
    "\n",
    "    # Concat'em\n",
    "    frames = [df_train, df_test]\n",
    "    df_train_and_test = pd.concat(frames)\n",
    "\n",
    "    # Rename columns to deepmoji's format.\n",
    "    df_train_and_test.rename({'text': 'texts', 'label':'info'}, axis=1, inplace=True)\n",
    "    \n",
    "    # Modify info column values 'str' -> {'label':label}\n",
    "    df_train_and_test['info'] = df_train_and_test['info'].apply(lambda x: {'label' : x})\n",
    "    \n",
    "    # Add the extra index columns\n",
    "    extra_index_cols =  ['train_ind', 'test_ind', 'val_ind']\n",
    "    df_train_and_test = df_train_and_test.join(pd.DataFrame(columns=extra_index_cols)) #.fillna(np.nan)\n",
    "    \n",
    "\n",
    "    # 2. Construct dict for pickeling.\n",
    "    # Get indeces\n",
    "\n",
    "    # Split original train data into train and validation sets.\n",
    "    org_train_ind = (range(0, len(df_train)))\n",
    "\n",
    "    train_val_split = train_test_split(org_train_ind, test_size=0.1)\n",
    "    train_ind = train_val_split[0]\n",
    "    val_ind = train_val_split[1]\n",
    "\n",
    "    \n",
    "\n",
    "    # Add new train indeces.\n",
    "    #df_train_and_test['train_ind'] = df_train_and_test['train_ind'].add(train_ind, ignore_index=True) #doesnt work.\n",
    "    #df_train_and_test['train_ind'] = pd.Series(train_ind) # this used to work..\n",
    "    df_train_and_test.iloc[len(val_ind):len(org_train_ind) : , 2] = train_ind # start at validation indec, run until test index.\n",
    "\n",
    "    # Add val indeces\n",
    "    #df_train_and_test['val_ind'] = pd.Series(val_ind) # this used to work.\n",
    "    df_train_and_test.iloc[:len(val_ind) : , 4] = val_ind\n",
    "\n",
    "    # Add test indeces\n",
    "    #test_ind = range(len(train_ind), len(df_train_and_test)) # this used to work.\n",
    "    test_ind = range(len(org_train_ind), len(df_train_and_test)) # new appraoch\n",
    "\n",
    "    #df_train_and_test.loc[len(train_ind), 'test_ind'] = test_ind # doesnt work ‚ùå\n",
    "    #df_train_and_test.iloc[len(train_ind) : , 3] =  test_ind # start at end of new train index, run to the end.. ‚úî \n",
    "    df_train_and_test.iloc[len(org_train_ind) : , 3] =  test_ind\n",
    "    #df_train_and_test.insert(loc=len(train_ind), column='test_ind', value=test_ind, allow_duplicates=False) # doesnt work ‚ùå\n",
    "    \n",
    "\n",
    "    # Check duplicates\n",
    "    #dupes = df_train_and_test.duplicated(subset=['train_ind'])\n",
    "\n",
    "    # 3. Pickle the dict.\n",
    "\n",
    "\n",
    "    output = df_train_and_test.to_dict('list')\n",
    "    \n",
    "    with open('../Data/picklemania/olid/olid_lvl_c_data.pkl', 'wb') as handle:\n",
    "       pickle.dump(output , handle)\n",
    "\n",
    "    # This is just for testing.\n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function call\n",
    "\n",
    "# TRAC\n",
    "test = pickle_data('../Data/OLID_full/converted/levelc/train.tsv' , '../Data/OLID_full/converted/levelc/test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13453\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   texts              info  \\\n",
       "0      @USER She should ask a few native Americans wh...    {'label': nan}   \n",
       "1      @USER @USER Go home you‚Äôre drunk!!! @USER #MAG...  {'label': 'IND'}   \n",
       "2      Amazon is investigating Chinese employees who ...    {'label': nan}   \n",
       "3      @USER Someone should'veTaken\" this piece of sh...    {'label': nan}   \n",
       "4      @USER @USER Obama wanted liberals &amp; illega...    {'label': nan}   \n",
       "...                                                  ...               ...   \n",
       "13448  #StopEtchecopar? Fuck you all üñïüñïüñïüñïüñï Que florez...    {'label': nan}   \n",
       "13449  #Antifa are mentally unstable cowards, pretend...    {'label': nan}   \n",
       "13450  @USER @USER And Browning looked like dog shit ...    {'label': nan}   \n",
       "13451                All two of them taste like ass. URL    {'label': nan}   \n",
       "13452  #DespicableDems lie again about rifles. Dem Di...    {'label': nan}   \n",
       "\n",
       "       train_ind  test_ind  val_ind  \n",
       "0            NaN       NaN   5466.0  \n",
       "1            NaN       NaN    956.0  \n",
       "2            NaN       NaN   4759.0  \n",
       "3            NaN       NaN   5394.0  \n",
       "4            NaN       NaN   9465.0  \n",
       "...          ...       ...      ...  \n",
       "13448        NaN   13448.0      NaN  \n",
       "13449        NaN   13449.0      NaN  \n",
       "13450        NaN   13450.0      NaN  \n",
       "13451        NaN   13451.0      NaN  \n",
       "13452        NaN   13452.0      NaN  \n",
       "\n",
       "[13453 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>texts</th>\n      <th>info</th>\n      <th>train_ind</th>\n      <th>test_ind</th>\n      <th>val_ind</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@USER She should ask a few native Americans wh...</td>\n      <td>{'label': nan}</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5466.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@USER @USER Go home you‚Äôre drunk!!! @USER #MAG...</td>\n      <td>{'label': 'IND'}</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>956.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Amazon is investigating Chinese employees who ...</td>\n      <td>{'label': nan}</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4759.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n      <td>{'label': nan}</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5394.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n      <td>{'label': nan}</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9465.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13448</th>\n      <td>#StopEtchecopar? Fuck you all üñïüñïüñïüñïüñï Que florez...</td>\n      <td>{'label': nan}</td>\n      <td>NaN</td>\n      <td>13448.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13449</th>\n      <td>#Antifa are mentally unstable cowards, pretend...</td>\n      <td>{'label': nan}</td>\n      <td>NaN</td>\n      <td>13449.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13450</th>\n      <td>@USER @USER And Browning looked like dog shit ...</td>\n      <td>{'label': nan}</td>\n      <td>NaN</td>\n      <td>13450.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13451</th>\n      <td>All two of them taste like ass. URL</td>\n      <td>{'label': nan}</td>\n      <td>NaN</td>\n      <td>13451.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13452</th>\n      <td>#DespicableDems lie again about rifles. Dem Di...</td>\n      <td>{'label': nan}</td>\n      <td>NaN</td>\n      <td>13452.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>13453 rows √ó 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "#test.keys()\n",
    "#test['texts'][:10]\n",
    "\n",
    "#print(' length', len(test['texts']))\n",
    "\n",
    "# Check if things align. (‚úî)\n",
    "\n",
    "#print(test['texts'][-1])\n",
    "#print(test['info'][-1:])\n",
    "#print(test['train_ind'][-1:])\n",
    "\n",
    "print(len(test['train_ind']))\n",
    "\n",
    "df_test = pd.DataFrame(test)\n",
    "df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './olid_lvl_c_data.pkl'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-46c8da5fd649>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# inspect pickle file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./olid_lvl_c_data.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './olid_lvl_c_data.pkl'"
     ]
    }
   ],
   "source": [
    "# inspect pickle file\n",
    "\n",
    "with open('../Data/picklemania/gibert/', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "print(len(data), type(data), data.keys())\n",
    "\n",
    "print(len(data['texts']))\n",
    "\n",
    "print()\n",
    "\n",
    "data\n",
    "#data['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}